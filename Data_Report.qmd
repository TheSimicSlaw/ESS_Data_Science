---
title: "Analyzing the Relationship Between Social Trust and the Importance of Democracy Using European Social Survey Data (2020)"
authors: Group 20 - Emma ‘Ais’ Burniston, Ryan Camp, Jacqueline Sanchez, Tan Tran, Samuel Mlinka
mainfont: Palatino Linotype
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

### Introduction (Ais):

In the past decade, there has been a great deal of changes within the political realm across the world, namely pushes towards governments that are more right-wing and authoritarian in nature. The causes of this shift are of course, multi-faceted, and express themselves differently across the world. In Turkey, there is a notable ethnonationalist element, concurrent with the country’s origins as a modern state (Kushner 1997). For Brazil, indigenous communities and their sovereign rights come into conflict with the economic deprivation of non-indigenous citizens, leading to distinct anti-Indian rhetoric in their far-right politics. (Conner 1995) In El Salvador, rusted-shut political machines and de-facto gang control produced such frustration among voters that they resorted to mass incarceration at an unprecedented scale. (Kelly et al., 2025) However the sense of lacking inertia is one of the more ubiquitous motivators of the rightward movement worldwide, and especially in Europe. Doing a numeric analysis on lack of interest is somewhat difficult, therefore we must consider what the inverse would be, that is trust in social institutions and human nature. Fortunately, the European Social Survey (ESS) Round 10, conducted in 2020, has an entire module based around this subject, and is available for our analytical use.

For the purposes of our project, we will be looking at the relationship between measures of social trust and the importance of democracy in six regions of Europe: Western Continental Europe, Central Europe, Southern Europe, Eastern Europe, the Nordics and the British Isles. We have used data from ESS 10, conducted by the Core Scientific Team (CST) based in the University of London. Our objective is to discover if there is a meaningful/significant relationship between these predictors and the importance of democracy using linear discriminant analysis and decision trees. Comparisons between these regions using multiple approaches may provide insight into the continental differences in political thought.

Our analysis focused on 9 predictors of social trust: *ppltrst, pplhlp, pplfair, trstprl, trstplt, trstlgl, trstplc, trstprt,* and *trstsci,* and the response variable implvdm - importance of democracy for \[country\]. All were measured on a 11-step Likert scale (0-10), but the response variable was partitioned into categories of Low, Medium or High, as their relative properties are of more interest than predicting the exact numeric response. The data was cleaned and preprocessed split into training and testing sets, then linear discriminant analysis and decision tree models were used to identify relationships with reported importance of democracy. We evaluated the performance of each model and discussed their relative strengths and limitations. Comparing LDA and decision trees will hopefully allow a more complete understanding of democracy’s relative worth within Europe.

### Linear Discriminant Analysis Model (Ais, Samuel):

Linear Discriminant Analysis (LDA) is the parametric model best suited to classification problems with more than two responses. As with any parametric method, it comes with assumptions. 1) The classes can be described using a multivariate normal distribution. 2) The classes share the same covariance matrix. 3) The observations are independent of each other. LDA’s benefits are in its simplicity and scalability, while LDA’s weaknesses lie in assumption 2 and its sensitivity to outliers/extreme values.\

The formula for determining the linear discriminant score for class i is as follows:\

$$g_i(x) = x^T \Sigma^{-1} \mu_i - \frac{1}{2} \mu_i^T \Sigma^{-1} \mu_i + \ln(P(\omega_i))$$

Where x is the input feature vector, μi is the mean vector of class i, Σ is the shared covariance matrix and P(ωi​) is the prior probability of class i.\

Our reasoning for using linear discriminant variance on classes derived from the implvdm - Low (0-4), Medium (5-6), and High (7-10) - henceforth dem_imp, rather than splitting at 5 for logistic regression is that higher point Likert scales are designed to measure granularity, especially within the context of social science research. Our objective was to determine which factors were useful for predicting dem_imp and whether they varied based on regionality.\
Excerpt from code:\

[Add code here]

The test accuracies in the order given above were 0.8914, 0.8278, 0.9465, 0.8688, 0.8491, and 0.8713.\

The significance of variables was slightly different between the different regions. In Central Europe, Eastern Europe, the Nordics, and Southern Europe the most significant factor was “trst_sci”. However, in the UK the most significant variable was “trst_lgl”. And for Western Central Europe the most significant factor was “pplfair”.\

K-fold cross validation was used to determine the accuracy of the model.\

Results of the 5 fold cross validation :\
Central Europe : \~89.24%\
Eastern Europe : \~82.11%\
Nordics : \~94.16%\
Southern Europe : \~86.65%\
UK : \~86.22%\
Western Central Europe : \~87.67%\

The overall accuracy of the K-fold was \~87.68%.\

### Decision Tree & Random Forest Model (Tan, Jacqueline, Ryan):

### Decision Trees
A decision tree is a supervised machine learning model used for classification or regression tasks. It works by splitting the data into branches based on feature values, forming a tree-like structure where each internal node represents a decision rule, and each leaf node represents an outcome. The goal is to create splits that best separate the target classes, making the model easy to interpret and visualize. Decision trees are particularly useful for understanding which features are most important in predicting the outcome.

In our function, the dataset is first balanced across classes using stratified sampling, ensuring equal representation of each class. A stratified train-test split is then applied based on a user-defined ratio (default 70/30). The model is trained on the training set and evaluated on the test set using a confusion matrix and accuracy metric. The resulting tree structure is visualized to aid interpretation, and the function returns performance metrics for further analysis.

Excerpt for Code: 

min_n <- min(table(data$dem_imp))
  data_bal <- data %>%
    group_by(dem_imp) %>%
    slice_sample(n = min_n) %>%
    ungroup()

train_idx <- createDataPartition(data_bal$dem_imp, p = split_ratio, list = FALSE)
  train_data <- data_bal[train_idx, ]
  test_data <- data_bal[-train_idx, ]

tree_model <- tree(dem_imp ~ ppltrst + pplhlp + pplfair + trstprl + trstlgl +
                       trstplc + trstplt + trstprt + trstsci,
                     data = train_data)
  predictions <- predict(tree_model, test_data, type = "class")
  conf_matrix <- table(Predicted = predictions, Actual = test_data$dem_imp)


```{r}
#| echo: false

# ================================
# Libraries
# ================================

library(tree)
library(caret)
library(dplyr)
library(randomForest)
library(doParallel)


# ================================
# Functions
# ================================

factoring <- function(df) {
  vars <- c("pplfair", "pplhlp", "ppltrst", "trstprl", "trstlgl", "trstplc", 
            "trstplt", "trstprt", "trstsci")
  for (v in vars) {
    df[[v]] <- as.factor(df[[v]])
  }
  return(df)
}

categorize_democracy <- function(df) {
  df$dem_imp <- ifelse(df$implvdm < 5, "Low",
                       ifelse(df$implvdm >= 7, "High", "Medium"))
  df$dem_imp <- as.factor(df$dem_imp)
  #df$implvdm <- NULL
  return(df)
}

prepare_data_split <- function(data, split_ratio = 0.7, seed = 1) {
  set.seed(seed)
  min_n <- min(table(data$dem_imp))
  data_bal <- data %>%
    group_by(dem_imp) %>%
    slice_sample(n = min_n) %>%
    ungroup()
  idx <- createDataPartition(data_bal$dem_imp, p = split_ratio, list = FALSE)
  train_data <- data_bal[idx, ]
  test_data <- data_bal[-idx, ]
  return(list(train = train_data, test = test_data))
}

decision_tree_model <- function(train_data, test_data, region_name = "Region") {
  tree_model <- tree(dem_imp ~ ppltrst + pplhlp + pplfair + trstprl + trstlgl +
                       trstplc + trstplt + trstprt + trstsci,
                     data = train_data)
  
  predictions <- predict(tree_model, test_data, type = "class")
  conf_matrix <- table(Predicted = predictions, Actual = test_data$dem_imp)
  accuracy <- mean(predictions == test_data$dem_imp)
  
  cat("\n=== Confusion Matrix for", region_name, "===\n")
  print(conf_matrix)
  cat(sprintf("\nAccuracy for %s: %.2f%%\n", region_name, accuracy * 100))
  
  cat("\nDecision Tree for", region_name, ":\n")
  plot(tree_model)
  text(tree_model, pretty = 0)
  
  return(tree_model)
}

get_prune_chart <- function(tree_model, train_data_input, test_data, region_name = "Region") {
  assign("train_data", train_data_input, envir = .GlobalEnv)
  prune_results <- cv.tree(tree_model, FUN = prune.misclass)
  rm(train_data, envir = .GlobalEnv)
  
  par(mfrow = c(1, 2))
  plot(prune_results$size, prune_results$dev, type = "b", main = "Tree Size vs Deviance")
  plot(prune_results$k, prune_results$dev, type = "b", main = "Complexity Param vs Deviance")
  
  best_size <- prune_results$size[which.min(prune_results$dev)]
  pruned_tree <- prune.misclass(tree_model, best = best_size)
  
  pruned_preds <- predict(pruned_tree, test_data, type = "class")
  conf_matrix <- table(Predicted = pruned_preds, Actual = test_data$dem_imp)
  accuracy <- mean(pruned_preds == test_data$dem_imp)
  
  cat("\n=== PRUNED Confusion Matrix for", region_name, "===\n")
  print(conf_matrix)
  cat(sprintf("\nPruned Accuracy for %s: %.2f%%\n", region_name, accuracy * 100))
  cat("\nPruned Tree for", region_name, ":\n")
  plot(pruned_tree)
  text(pruned_tree, pretty = 0)
  
  return(pruned_tree)
}

run_pipeline <- function(region_data, region_name) {
  split <- prepare_data_split(region_data)
  tree <- decision_tree_model(split$train, split$test, region_name)
  pruned <- get_prune_chart(tree, split$train, split$test, region_name)
  return(list(tree = tree, pruned = pruned))
}

random_forest_model <- function(region_data, region_name) {
  set.seed(1)
  forest.train = sample(1:nrow(region_data), nrow(region_data)/2)
  region.xtest = region_data[-forest.train, c(2:4, 6:11)]
  region.ytest = region_data[-forest.train, 12]
  region.forest = randomForest(dem_imp ~ ppltrst + pplhlp + pplfair + trstprl +
                                 trstlgl + trstplc + trstplt + trstprt +
                                 trstsci, xtest = region.xtest, 
                               ytest = region.ytest, subset = forest.train, 
                               mtry = 3, keep.forest = TRUE, 
                               data = region_data, importance = TRUE)
  cat(region_name, "Random Forest Variable Importance")
  print(importance(region.forest))
  varImpPlot(region.forest, main = region_name)
  
  region.pred <- predict(region.forest, region_data[-forest.train,])
  conf_mat <- table(region.pred, region.ytest)
  conf_acc <- (conf_mat[1,1] + conf_mat[2,2] + conf_mat[3,3])/sum(conf_mat)
  cat("\n=== Random Forest Confusion Matrix for", region_name, "===\n")
  print(conf_mat)
  cat(sprintf("\nRandom Forest Accuracy for %s: %.2f%%\n", region_name, conf_acc*100))
  
  return(region.forest)
}
 ,m  ,..

# ================================
# Pre-Plotting Processing
# ================================

setwd("./non-parametric")

central <- read.csv("central_europe_clean_csv.csv")
eastern <- read.csv("eastern_europe_clean_csv.csv")
nordics <- read.csv("nordics_clean_csv.csv")
southern <- read.csv("southern_europe_clean_csv.csv")
uk <- read.csv("uk_clean_csv.csv")
west_central <- read.csv("wce_clean_csv.csv")

setwd("..")

central_2 <- categorize_democracy(factoring(central))
eastern_2 <- categorize_democracy(factoring(eastern))
nordics_2 <- categorize_democracy(factoring(nordics))
southern_2 <- categorize_democracy(factoring(southern))
uk_2 <- categorize_democracy(factoring(uk))
west_central_2 <- categorize_democracy(factoring(west_central))
```



#### Central Europe

```{r}
#| echo: false
central.data_split <- prepare_data_split(central_2)

central.tree <- decision_tree_model(central.data_split$train, central.data_split$test, "Central Europe (70/30)")

central.pruned_tree <- get_prune_chart(central.tree, central.data_split$train, central.data_split$test, "Central Europe (70/30)")


central.data_split <- prepare_data_split(central_2, 0.8)

central.tree <- decision_tree_model(central.data_split$train, central.data_split$test, "Central Europe (80/20)")

central.pruned_tree <- get_prune_chart(central.tree, central.data_split$train, central.data_split$test, "Central Europe (80/20)")
```


In Central Europe, both the 70/30 and 80/20 splits produced moderately complex trees with an accuracy of 48.6% and 52.4%, respectively. With pruning, the accuracies were 49.5% and 52.4%, respectively. The minimal differences suggest that pruning helped simplify the model, but didn’t significantly boost performance. This indicates that Central Europe’s predictor space contains enough variation for meaningful class separation. The most influential features were trstprl and trstsci, followed by pplfair and pplhlp. Notably, the distributions of these trust variables are skewed toward higher values, which likely made it easier for the model to differentiate between classes—especially in the 80/20 configuration. Overall, the model struggles to classify the “Low” and “Medium”, suggesting there is potential underfitting or overlapping features. 


```{r}
#| echo: false
central.random_forest <- random_forest_model(central_2, "Central Europe")
```

It's rather interesting that trstplt, trstprt, and trstprt have the highest mean decrease accuracy and the lowest mean decrease gini. Given that the gini plot is rather even while the accuracy plot is more staggered, I'd be inclined to say that the mean decrease accuracy and thus trstplt, trstprt, and trstprt are ore significant.

#### Eastern Europe

```{r}
#| echo: false
eastern.data_split <- prepare_data_split(eastern_2)

eastern.tree <- decision_tree_model(eastern.data_split$train, eastern.data_split$test, "Eastern Europe (70/30)")

eastern.pruned_tree <- get_prune_chart(eastern.tree, eastern.data_split$train, eastern.data_split$test, "Eastern Europe (70/30)")

eastern.data_split <- prepare_data_split(eastern_2, 0.8)

eastern.tree <- decision_tree_model(eastern.data_split$train, eastern.data_split$test, "Eastern Europe (80/20)")

eastern.pruned_tree <- get_prune_chart(eastern.tree, eastern.data_split$train, eastern.data_split$test, "Eastern Europe (80/20)")

```

Eastern Europe showed modest improvements in accuracy after pruning for the 70/30 split, rising from 40.2% to 44.%. On the other hand, pruning reduced the accuracy from 44.2% to 43.6% for the 80/20 split. The model primarily splits on trstprl and trstsci, though the distribution plots reveal more constrained variation in key features like pplhlp and ppltrst. This limited variability may have made it harder for the model to confidently distinguish between levels of democratic importance. While pruning did help to reduce noise, the lack of strong separation in the data itself might be holding back overall performance.


```{r}
#| echo: false
eastern.random_forest <- random_forest_model(eastern_2, "Eastern Europe")
```

Here, trstsci and trstplc are the highest for mean decrease accuracy and both in the top three for the mean decrease gini. This fits with the decision trees above, as trstsci and trstplc are consistently the main two splits.


#### Southern Europe

```{r}
#| echo: false
southern.data_split <- prepare_data_split(southern_2)

southern.tree <- decision_tree_model(southern.data_split$train, southern.data_split$test, "Southern Europe (70/30)")

southern.pruned_tree <- get_prune_chart(southern.tree, southern.data_split$train, southern.data_split$test, "Southern Europe (70/30)")


southern.data_split <- prepare_data_split(southern_2, 0.8)

southern.tree <- decision_tree_model(southern.data_split$train, southern.data_split$test, "Southern Europe (80/20)")

southern.pruned_tree <- get_prune_chart(southern.tree, southern.data_split$train, southern.data_split$test, "Southern Europe (80/20)")
```

In Southern Europe, pruning did not improve the model’s performance. Accuracy stayed at 41.3% (70/30) and  39.8% (80/20). In fact, pruning the tree suggests the model couldn’t find meaningful splits. This may reflect the region’s heavily skewed distributions, where trust levels in variables like trstprl and pplfair are generally low. Although class balancing was applied during training, the test set distributions may have remained skewed, limiting generalizability.

```{r}
#| echo: false
southern.random_forest <- random_forest_model(southern_2, "Southern Europe")
```

#### West-Central Europe

```{r}
#| echo: false
west_central.data_split <- prepare_data_split(west_central_2)

west_central.tree <- decision_tree_model(west_central.data_split$train, west_central.data_split$test, "West-Central Europe (70/30)")

west_central.pruned_tree <- get_prune_chart(west_central.tree, west_central.data_split$train, west_central.data_split$test, "West-Central Europe (70/30)")


west_central.data_split <- prepare_data_split(west_central_2, 0.8)

west_central.tree <- decision_tree_model(west_central.data_split$train, west_central.data_split$test, "West-Central Europe (80/20)")

west_central.pruned_tree <- get_prune_chart(west_central.tree, west_central.data_split$train, west_central.data_split$test, "West-Central Europe (80/20)")
```

Western Central Europe had the strongest overall performance, with accuracy stayed 54.44% after pruning in the 70/30 split, but increased to 50% in the 80/20 split. Minimal improvement after pruning suggests the tree had already captured most of the useful signal without overfitting, and the predictor space was not overly complex. The trees here were well-structured and used a balanced mix of predictors like trstsci, pplhlp, and pplfair. The relatively even distribution of trust scores across the population likely made it easier for the model to learn meaningful patterns. This region’s results suggest that when trust measures are well-distributed, decision trees can perform quite well.

```{r}
#| echo: false
west_central.random_forest <- random_forest_model(west_central_2, "West-Central Europe")
```

#### Nordics

```{r}
#| echo: false
nordics.data_split <- prepare_data_split(nordics_2)

nordics.tree <- decision_tree_model(nordics.data_split$train, nordics.data_split$test, "Nordics (70/30)")

nordics.pruned_tree <- get_prune_chart(nordics.tree, nordics.data_split$train, nordics.data_split$test, "Nordics (70/30)")

nordics.data_split <- prepare_data_split(nordics_2, 0.8)

nordics.tree <- decision_tree_model(nordics.data_split$train, nordics.data_split$test, "Nordics (80/20)")

nordics.pruned_tree <- get_prune_chart(nordics.tree, nordics.data_split$train, nordics.data_split$test, "Nordics (80/20)")
```

In the Nordic countries, pruning led to noticeable gains in accuracy—from 39.2% to 51.0% (70/30) and from 36.4% to 42.4% (80/20). Despite relatively simple pruned trees, performance improved, likely due to the consistent and high-trust distributions seen in features like pplhlp and trstsci. These predictors likely provided enough signal for the model once overfitting was reduced. However, the skew toward high values may also make it harder to draw distinctions between the Medium and High categories of democratic importance, which could be why the improvement levels off below 55%.


```{r}
#| echo: false
nordics.random_forest <- random_forest_model(nordics_2, "Nordics")
```



#### United Kingdom

```{r}
#| echo: false
uk.data_split <- prepare_data_split(uk_2)

uk.tree <- decision_tree_model(uk.data_split$train, uk.data_split$test, "United Kingdom (70/30)")

uk.pruned_tree <- get_prune_chart(uk.tree, uk.data_split$train, uk.data_split$test, "United Kingdom")


uk.data_split <- prepare_data_split(uk_2, 0.8)

uk.tree <- decision_tree_model(uk.data_split$train, uk.data_split$test, "United Kingdom (80/20)")

uk.pruned_tree <- get_prune_chart(uk.tree, uk.data_split$train, uk.data_split$test, "United Kingdom (80/20)")
```

The UK had the lowest overall performance, with accuracy increasing from 33.3% to 46.2% (70/30) and reaching only 39.2% (80/20) after pruning. The trees were fairly shallow, and the splits leaned heavily on pplhlp, ppltrst, and trstprl. The relatively uniform distributions of these features across trust levels and the lack of data likely contributed to the model’s struggle to separate classes effectively. 

```{r}
#| echo: false
uk.random_forest <- random_forest_model(uk_2, "United Kingdom")
```

------------------------------------------------------------------------
